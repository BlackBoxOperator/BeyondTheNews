{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/bc/jsywdsrx5jd3sh37pspc1mhh0000gn/T/jieba.cache\n",
      "Loading model cost 0.974 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "  0%|          | 198/99917 [00:00<00:50, 1970.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "appending title to document...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99917/99917 [00:39<00:00, 2556.44it/s]\n",
      "  1%|          | 1142/99917 [00:00<00:17, 5589.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    building corpus vector space...\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99917/99917 [00:19<00:00, 5148.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "corpus vector space - ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import *\n",
    "import numpy as np\n",
    "import time, jieba, os, json, csv, re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from score_functions import twostage\n",
    "from itertools import starmap\n",
    "from bm25 import BM25Transformer\n",
    "\n",
    "queryFile = os.path.join('..', 'data', 'QS_1.csv')\n",
    "stopwordFile = os.path.join('..', 'data', \"stopword.txt\")\n",
    "outputFile = os.path.join('..', 'submit', 'current.csv')\n",
    "titleJson = os.path.join('..', 'data', \"title.json\")\n",
    "\n",
    "cut_method = jieba.cut_for_search\n",
    "tokenFile = os.path.join('..', 'tokens', 'search_dict_token.txt')\n",
    "tokeyFile = os.path.join('..', 'tokens', 'search_dict_tokey.txt')\n",
    "queryDictFile = os.path.join('..', 'data', 'dict.txt')\n",
    "\n",
    "jieba.load_userdict(queryDictFile)\n",
    "\n",
    "def retain_chinese(line):\n",
    "    return re.compile(r\"[^\\u4e00-\\u9fa5]\").sub('', line).replace('臺', '台')\n",
    "\n",
    "def get_screen_len(line):\n",
    "    chlen = len(retain_chinese(line))\n",
    "    return (len(line) - chlen) + chlen * 2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    stopwords = open(stopwordFile, 'r').read().split()\n",
    "    queries = dict([row for row in csv.reader(open(queryFile, 'r'))][1:])\n",
    "    titles = json.load(open(titleJson, \"r\"))\n",
    "\n",
    "    trim = lambda f: [t.strip() for t in f if t.strip()]\n",
    "    token = trim(open(tokenFile).read().split('\\n'))#[:5000]#[:301]\n",
    "    tokey = trim(open(tokeyFile).read().split('\\n'))#[:5000]#[:301]\n",
    "\n",
    "    # append title to doc\n",
    "    print(\"\"\"\n",
    "appending title to document...\n",
    "\"\"\")\n",
    "\n",
    "    title_weight = 1\n",
    "\n",
    "    for i, key in enumerate(tqdm(tokey)):\n",
    "        title = retain_chinese(titles.get(key, '')).strip()\n",
    "        if title and title != \"Non\":\n",
    "            title_token = ' {}'.format(' '.join([w for w\n",
    "                in cut_method(title) if w not in stopwords])) * title_weight\n",
    "            token[i] += title_token\n",
    "            #print('+= ' + title_token)\n",
    "\n",
    "    if len(token) != len(tokey):\n",
    "        print('token len sould eq to tokey len')\n",
    "        exit(0)\n",
    "\n",
    "    bm25 = BM25Transformer()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    print(\"\"\"\n",
    "    building corpus vector space...\n",
    "        \"\"\")\n",
    "\n",
    "    doc_tf = vectorizer.fit_transform(tqdm(token))\n",
    "\n",
    "    bm25.fit(doc_tf)\n",
    "    doc_bm25 = bm25.transform(doc_tf)\n",
    "\n",
    "    print('\\ncorpus vector space - ok\\n')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'北市'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmptoken = [t.split(\" \") for t in token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tt = np.array(tmptoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 2),\n",
       " (15, 2),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 2),\n",
       " (24, 2),\n",
       " (25, 2),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 2),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 2),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 3),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 2),\n",
       " (62, 4),\n",
       " (63, 2),\n",
       " (64, 2),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 5),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 3),\n",
       " (82, 3),\n",
       " (83, 2),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 3),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 1),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 1),\n",
       " (99, 1),\n",
       " (100, 1),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 1),\n",
       " (104, 1),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (107, 1),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 1),\n",
       " (111, 1),\n",
       " (112, 1),\n",
       " (113, 1),\n",
       " (114, 2),\n",
       " (115, 2),\n",
       " (116, 3),\n",
       " (117, 2),\n",
       " (118, 1),\n",
       " (119, 1),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 1),\n",
       " (127, 1),\n",
       " (128, 1),\n",
       " (129, 2),\n",
       " (130, 1),\n",
       " (131, 3),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 2),\n",
       " (138, 1),\n",
       " (139, 1),\n",
       " (140, 2),\n",
       " (141, 1),\n",
       " (142, 1),\n",
       " (143, 1),\n",
       " (144, 1),\n",
       " (145, 3),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (148, 1),\n",
       " (149, 3),\n",
       " (150, 1),\n",
       " (151, 2),\n",
       " (152, 2),\n",
       " (153, 1),\n",
       " (154, 1),\n",
       " (155, 1),\n",
       " (156, 1),\n",
       " (157, 1),\n",
       " (158, 1),\n",
       " (159, 2),\n",
       " (160, 1),\n",
       " (161, 1),\n",
       " (162, 1),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 2),\n",
       " (166, 3),\n",
       " (167, 3),\n",
       " (168, 3),\n",
       " (169, 2),\n",
       " (170, 1),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 1),\n",
       " (175, 1),\n",
       " (176, 14),\n",
       " (177, 1),\n",
       " (178, 1),\n",
       " (179, 6)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(tt)\n",
    "corpus = [dictionary.doc2bow(text) for text in tt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "# bigram_transformer = Phrases(tt)\n",
    "# model = Word2Vec(bigram_transformer[tt], min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2786895119, 2810294600)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.train(tt, total_examples=len(tt), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# model.save(\"./model.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"./model.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosSim(v1,v2):\n",
    "        return np.dot(v1,v2)/(np.sqrt((v1*v1).sum())*np.sqrt((v2*v2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.5576663 ,  1.1192425 ,  1.5257726 , -2.9596596 , -2.0510314 ,\n",
       "        3.5871036 ,  0.413788  , -0.7508377 , -2.3556578 , -3.4461    ,\n",
       "        1.4267297 , -0.21273573, -1.4951439 , -1.0722139 ,  5.6647687 ,\n",
       "       -0.2764437 ,  0.51061463,  1.975527  ,  1.1960163 , -3.9791973 ,\n",
       "        2.8652143 , -3.17638   ,  4.322652  ,  1.3153908 , -3.5056844 ,\n",
       "        3.4308507 ,  0.60897857,  3.1287992 , -3.9260778 ,  4.308882  ,\n",
       "        4.4180827 ,  2.2497222 ,  2.1650782 , -0.40933117,  2.160309  ,\n",
       "       -3.3779986 , -2.163458  ,  1.3997703 , -1.3266774 ,  0.19308692,\n",
       "        3.073745  ,  0.2268004 , -3.1317358 ,  0.4952132 , -0.21560237,\n",
       "       -3.2652175 ,  3.0387008 ,  1.5051608 , -4.8596516 , -1.9798121 ,\n",
       "        1.7529259 ,  1.5999805 ,  4.234009  , -1.6427894 , -0.666432  ,\n",
       "        0.03241699,  0.38046533,  4.646892  ,  1.2129058 , -1.4001262 ,\n",
       "       -1.7795942 , -5.3961287 , -0.7493347 ,  1.7067924 , -3.6159568 ,\n",
       "        4.4780188 , -1.8129102 ,  2.1592896 ,  0.10626329,  1.8269609 ,\n",
       "       -3.2907438 , -1.5351142 , -0.42142057,  1.8612627 ,  5.106374  ,\n",
       "       -0.07211841, -2.1414425 ,  3.0157754 ,  3.485475  ,  2.2636135 ,\n",
       "        3.7704675 ,  6.7403674 ,  4.3512545 , -3.0309675 , -0.9944717 ,\n",
       "        1.3962456 ,  2.3655694 ,  1.5371172 , -1.3484615 , -4.6729755 ,\n",
       "        1.9290147 ,  2.5796647 , -1.8397576 , -2.5727687 ,  2.0010567 ,\n",
       "       -0.03138515, -1.1059047 , -0.46020284,  2.4178538 , -1.1084498 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"支持\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpVec = np.zeros((len(tmptoken),len(model.wv[tmptoken[0][0]])))\n",
    "for i in range(len(tmptoken)):\n",
    "    for j in range(len(tmptoken[i])):\n",
    "        tmpVec[i] += model.wv[tmptoken[i][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.39992290e+01,  6.82515966e+01, -1.36086924e+02, -3.71622288e+02,\n",
       "       -1.92190721e+02,  3.55742338e+02,  1.78321830e+02, -2.06715327e+01,\n",
       "        2.15504038e+02, -3.01888997e+02,  2.66091637e+02, -9.64820034e+01,\n",
       "       -4.01495618e+01,  3.30316797e+02,  3.16274124e+02,  1.46375351e+02,\n",
       "       -2.19227789e+02,  3.03366870e+02,  5.07105749e+01, -1.00007415e+02,\n",
       "        3.38908023e+02, -2.66795773e+02,  6.11915658e+01, -3.46390806e+01,\n",
       "       -1.95351017e+02, -3.04292442e+01, -1.28320564e+02,  1.42307523e+02,\n",
       "       -1.13359275e+02,  2.12413983e+02,  2.77743363e+01,  1.97688530e+02,\n",
       "        1.64288004e+02, -3.46583797e+02, -5.09196611e+01, -3.29654030e+02,\n",
       "       -2.77223392e+02,  3.57462996e+02, -4.57433445e+02,  4.50802575e+01,\n",
       "        5.96218434e+00,  1.59113969e+02, -4.86398919e+01,  2.82505946e+02,\n",
       "       -2.19712350e+02, -7.01310140e+01, -1.30387921e+02, -3.60528659e+02,\n",
       "       -6.04555957e+01,  1.83274254e+02,  3.63330139e+01, -6.13793417e+01,\n",
       "        3.00048565e+02, -2.36628573e+01, -4.12180672e+01, -1.70807481e+02,\n",
       "       -2.88005404e+02,  2.11206775e+02, -4.66067947e+02,  7.03767633e+01,\n",
       "       -1.10550861e+02, -8.49164801e+01,  1.69699403e+02,  1.31730147e+02,\n",
       "        1.04671651e+02,  1.68736206e+02,  1.43419800e+01,  7.78716738e+01,\n",
       "       -6.98175831e+01,  2.56097012e+02, -1.27354398e+02, -1.48023961e+02,\n",
       "       -9.29955371e-01,  1.19153070e+02,  4.17719802e+02, -6.72950685e+00,\n",
       "       -1.21822877e+02, -1.63662577e+02,  8.85091560e+01, -1.61373299e+02,\n",
       "       -3.70816775e+02,  1.58447772e+02,  2.74088637e+02,  2.27748539e+01,\n",
       "       -1.29922552e-01, -2.52674144e+01, -3.37663763e+01,  1.74122462e+02,\n",
       "        1.15771087e+02, -2.29663805e+01, -5.82658439e+01, -1.58991747e+02,\n",
       "        6.43682253e+01, -1.13612906e+02, -5.94907634e+00, -3.25782124e+01,\n",
       "       -3.38429614e+01, -7.95533171e+01,  5.90224038e+01,  1.21569053e+02])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpVec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 2208.69it/s]\n"
     ]
    }
   ],
   "source": [
    "qVec = np.zeros((len(queries),len(model.wv[tmptoken[0][0]])))\n",
    "for idx, q_id in enumerate(tqdm(queries)):\n",
    "            query = \" \".join([w for w in cut_method(queries[q_id].replace('臺', '台'))\n",
    "                                if w not in stopwords])\n",
    "\n",
    "            if '中國學生' in queries[q_id]:\n",
    "                query += ' 陸生 中生 大陸 學生'\n",
    "            if '證所' in queries[q_id]:\n",
    "                query += ' 證交稅 證交'\n",
    "            arrQ = str(query).split(\" \")\n",
    "            for i in range(len(arrQ)):\n",
    "                if not 'ECFA' == arrQ[i]:\n",
    "                    qVec[idx] += model.wv[arrQ[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.56810975,   4.00773245,  10.46025255, -17.80170852,\n",
       "        -4.08235262,  -1.69459467,  18.36074734,   5.39938687,\n",
       "        -9.23470613,  -9.42770457,  10.00529775, -16.03083272,\n",
       "        -3.14838126,  20.22437549,  -3.20608684,   0.93645483,\n",
       "         2.73487355,   4.40819226,   5.20818944,  17.2538532 ,\n",
       "        -4.44364959,   9.43236125,   8.26833332,   7.14559656,\n",
       "       -12.07207733,  -1.41059649,  -1.25651303,  -4.35024491,\n",
       "         4.96286345,   7.47638   ,   3.955522  ,  -1.12594549,\n",
       "        -3.11910835,  -4.110855  ,   3.38317847, -14.93816489,\n",
       "         2.42774957,  19.25571698, -15.27299738,  -4.42327173,\n",
       "        14.05682182,  -4.58578908,  -3.85652122,   2.89296973,\n",
       "        -5.85860375,  -2.53099322,  10.76760435,   3.80943641,\n",
       "        -3.24338518,  -2.05608559,   0.623032  ,  -4.30167055,\n",
       "        21.68389463,   9.63797355,  -2.69032785, -10.84806818,\n",
       "        -3.29620367,   8.74891186, -18.90895998,   9.19774485,\n",
       "        -9.11076358, -20.73060441,  -8.32248308,  -0.8199743 ,\n",
       "        -3.17854607,   9.32436234,  -8.4608658 ,   3.66526008,\n",
       "       -12.05998483,  -8.41932487,  -1.73014796,  -0.57823843,\n",
       "        -7.03455353,  -7.82751013,  19.2060014 ,  10.45281574,\n",
       "         8.51149935,  14.51458526,  -3.07961066,  -6.83213677,\n",
       "        15.82287487,  -0.76635104, -10.16021466,   3.18743514,\n",
       "        -6.84178343,   3.57937109,  -9.51091957,  -3.74242765,\n",
       "        -0.82663389,   5.65650871,   5.30722567,   6.78034186,\n",
       "        -1.68116977,  -8.66145444,  -0.94226199,   3.06070554,\n",
       "        15.21229362,   7.03562665,   4.91011789,   3.56961673])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qVec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30048128, -0.10606141,  0.22084023, ...,  0.17933733,\n",
       "         0.16357617,  0.21927577],\n",
       "       [ 0.21159386,  0.0025054 ,  0.06804307, ...,  0.20432211,\n",
       "         0.24401997,  0.09188853],\n",
       "       [ 0.3701137 ,  0.16762266,  0.48283084, ...,  0.31538718,\n",
       "         0.32561444,  0.3156602 ],\n",
       "       ...,\n",
       "       [ 0.30597629,  0.09100647,  0.22675069, ...,  0.3161897 ,\n",
       "         0.29753776,  0.21291079],\n",
       "       [ 0.13481879,  0.04326903,  0.20431324, ...,  0.15971502,\n",
       "        -0.02473682,  0.02022542],\n",
       "       [ 0.21906409,  0.06239096,  0.32394048, ...,  0.16355286,\n",
       "         0.13485201,  0.26400552]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.zeros((len(queries),len(tmptoken)))\n",
    "for i in range(len(queries)):\n",
    "    for j in range(len(tmptoken)):\n",
    "        scores[i][j] = cosSim(qVec[i],tmpVec[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ stage 0/5 ] Query1: 通姦 刑法 應該 除罪 除罪化\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:13<04:13, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query2: 應該 取消 機車 強制 二段 段式 二段式 左轉 待轉\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:25<03:56, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                                    \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query3: 支持 博弈 特區 台灣 合法 合法化\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:38<03:39, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                     \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query4: 中華 航空 空服 空服員 罷工 合理\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:48<03:14, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                     \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query5: 性交 交易 性交易 應該 合法 合法化\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [01:00<02:59, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                       \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query6: ECFA 早收 清單 達到 預期 成效\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [01:11<02:45, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                   \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query7: 應該 減免 證所 證所稅 證交稅 證交\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [01:22<02:28, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                       \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query8: 贊成 中油 觀塘 興建 第三 天然 天然氣 接收 接收站\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [01:32<02:13, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                                      \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query9: 支持 中國 學生 納入 健保 陸生 中生 大陸 學生\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [01:46<02:11, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                                  \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query10: 支持 台灣 中小 中小學 含 高職 專科 服儀 規定 含 髮 襪 鞋 給予 學生 自主\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [01:59<02:02, 12.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                                                              \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query11: 不 支持 使用 加密 貨幣\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [02:14<01:56, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                             \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query12: 不 支持 雜費 學雜費 調漲\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [02:26<01:42, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                               \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query13: 同意 政府 舉債 發展 前瞻 建設 計畫\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [02:38<01:28, 12.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                         \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query14: 支持 電競 列入 體育 競技\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [02:51<01:15, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                               \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query15: 反對 台鐵 東移 徵收 徵收案\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [03:04<01:03, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                 \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query16: 支持 陳 前 總統 保外 就醫\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [03:21<00:56, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query17: 年金 改革 應 取消 或應 調降 公教 軍公教 月退 優存 利率 十八 趴\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [03:36<00:42, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                                                     \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query18: 同意 動物 實驗\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n",
      "\u001b[F[ stage 3/5 ]\n",
      "\u001b[F[ stage 4/5 ]\n",
      "\u001b[F[ stage 5/5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [03:49<00:27, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                     \n",
      "\u001b[F\u001b[F\u001b[F\n",
      "[ stage 0/5 ] Query19: 油價 應該 凍漲 緩漲\n",
      "\u001b[F[ stage 1/5 ]\n",
      "\u001b[F[ stage 2/5 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    with open(outputFile, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        headers = ['Query_Index'] + ['Rank_{:03d}'.format(i) for i in range(1, 301)]\n",
    "        writer.writerow(headers)\n",
    "\n",
    "        for idx, q_id in enumerate(tqdm(queries)):\n",
    "\n",
    "\n",
    "            query = ' '.join([w for w in cut_method(queries[q_id].replace('臺', '台'))\n",
    "                                if w not in stopwords])\n",
    "\n",
    "            if '中國學生' in queries[q_id]:\n",
    "                query += ' 陸生 中生 大陸 學生'\n",
    "            if '證所' in queries[q_id]:\n",
    "                query += ' 證交稅 證交'\n",
    "\n",
    "            stages = [20, 40, 60, 80, 100]\n",
    "\n",
    "            init_bar = '[ stage 0/{} ] Query{}: {}'.format(len(stages), idx + 1, query)\n",
    "            print(init_bar)\n",
    "            qry_tf = vectorizer.transform([query])\n",
    "            qry_bm25 = bm25.transform(qry_tf)\n",
    "\n",
    "            sims = cosine_similarity(qry_bm25, doc_bm25)[0]\n",
    "            sims += scores[idx]\n",
    "            ranks = [(t, v) for (v, t) in zip(sims, tokey)]\n",
    "            ranks.sort(key=lambda e: e[-1], reverse=True)\n",
    "\n",
    "            for stage, fb_n in enumerate(stages):\n",
    "\n",
    "                print(\"\\033[F[ stage {}/{} ]\".format(stage + 1, len(stages)))\n",
    "\n",
    "                # relavance feedback stage 1\n",
    "                qry_bm25 = qry_bm25 + \\\n",
    "                         np.sum(doc_bm25[tokey.index(ranks[i][0])] * 0.5 \\\n",
    "                         for i in range(fb_n))\n",
    "\n",
    "\n",
    "                sims = cosine_similarity(qry_bm25, doc_bm25)[0]\n",
    "                sims += scores[idx]\n",
    "                ranks = [(t, v) for (v, t) in zip(sims, tokey)]\n",
    "                ranks.sort(key=lambda e: e[-1], reverse=True)\n",
    "\n",
    "            entry = [q_id] + [e[0] for e in ranks[:300]]\n",
    "            writer.writerow(entry)\n",
    "\n",
    "            print(\"\\033[F\" + ' ' * get_screen_len(init_bar))\n",
    "            print(\"\\033[F\" * 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
